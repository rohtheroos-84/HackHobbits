{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9146200,"sourceType":"datasetVersion","datasetId":5524489},{"sourceId":9971597,"sourceType":"datasetVersion","datasetId":6134770}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-08T02:52:56.415018Z","iopub.execute_input":"2025-03-08T02:52:56.415336Z","iopub.status.idle":"2025-03-08T02:52:56.695870Z","shell.execute_reply.started":"2025-03-08T02:52:56.415307Z","shell.execute_reply":"2025-03-08T02:52:56.695079Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"pip install timm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T02:52:57.234885Z","iopub.execute_input":"2025-03-08T02:52:57.235274Z","iopub.status.idle":"2025-03-08T02:53:01.808868Z","shell.execute_reply.started":"2025-03-08T02:52:57.235248Z","shell.execute_reply":"2025-03-08T02:53:01.807788Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.12)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.4.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.1+cu121)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.24.7)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (10.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\nimport timm\nfrom PIL import Image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T02:53:01.810155Z","iopub.execute_input":"2025-03-08T02:53:01.810421Z","iopub.status.idle":"2025-03-08T02:53:08.887722Z","shell.execute_reply.started":"2025-03-08T02:53:01.810400Z","shell.execute_reply":"2025-03-08T02:53:08.887099Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Directories for real and manipulated videos\nreal_videos_dir = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_original sequences\"\nmanipulated_videos_dir = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_manipulated_sequences/DFD_manipulated_sequences\"\n\n# Output directories for extracted frames\noutput_real_dir = \"/kaggle/working/frames/real\"\noutput_manipulated_dir = \"/kaggle/working/frames/manipulated\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T02:53:34.175925Z","iopub.execute_input":"2025-03-08T02:53:34.176264Z","iopub.status.idle":"2025-03-08T02:53:34.180039Z","shell.execute_reply.started":"2025-03-08T02:53:34.176237Z","shell.execute_reply":"2025-03-08T02:53:34.179142Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Ensure output directories exist\nos.makedirs(output_real_dir, exist_ok=True)\nos.makedirs(output_manipulated_dir, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T02:53:35.495091Z","iopub.execute_input":"2025-03-08T02:53:35.495360Z","iopub.status.idle":"2025-03-08T02:53:35.499409Z","shell.execute_reply.started":"2025-03-08T02:53:35.495340Z","shell.execute_reply":"2025-03-08T02:53:35.498667Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def extract_frames_from_videos(videos_dir, output_dir, label, max_videos=50):\n    video_files = [f for f in os.listdir(videos_dir) if f.endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n    video_files = video_files[:max_videos]  # Limit to max_videos\n\n    for video_file in video_files:\n        video_path = os.path.join(videos_dir, video_file)\n        cap = cv2.VideoCapture(video_path)\n        frame_count = 0\n        success, image = cap.read()\n\n        while success:\n            if frame_count % int(cap.get(cv2.CAP_PROP_FPS)) == 0:\n                frame_filename = f\"{label}_{video_file}_frame{frame_count // int(cap.get(cv2.CAP_PROP_FPS))}.jpg\"\n                frame_path = os.path.join(output_dir, frame_filename)\n                cv2.imwrite(frame_path, image)\n            success, image = cap.read()\n            frame_count += 1\n\n        cap.release()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T02:53:37.775482Z","iopub.execute_input":"2025-03-08T02:53:37.775798Z","iopub.status.idle":"2025-03-08T02:53:37.781601Z","shell.execute_reply.started":"2025-03-08T02:53:37.775772Z","shell.execute_reply":"2025-03-08T02:53:37.780514Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Extract frames from 100 real and 100 manipulated videos\nextract_frames_from_videos(real_videos_dir, output_real_dir, \"real\", max_videos=100)\nextract_frames_from_videos(manipulated_videos_dir, output_manipulated_dir, \"manipulated\", max_videos=100)\nprint(\"Frame extraction completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T02:53:40.971751Z","iopub.execute_input":"2025-03-08T02:53:40.972017Z","iopub.status.idle":"2025-03-08T03:04:24.963385Z","shell.execute_reply.started":"2025-03-08T02:53:40.971997Z","shell.execute_reply":"2025-03-08T03:04:24.962614Z"}},"outputs":[{"name":"stdout","text":"Frame extraction completed.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# !zip -r /kaggle/working/frames.zip frames","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T03:15:16.776415Z","iopub.execute_input":"2025-03-08T03:15:16.776726Z","iopub.status.idle":"2025-03-08T03:15:16.780383Z","shell.execute_reply.started":"2025-03-08T03:15:16.776702Z","shell.execute_reply":"2025-03-08T03:15:16.779486Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define image transformations with advanced augmentations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T03:15:19.936727Z","iopub.execute_input":"2025-03-08T03:15:19.937030Z","iopub.status.idle":"2025-03-08T03:15:20.014546Z","shell.execute_reply.started":"2025-03-08T03:15:19.937002Z","shell.execute_reply":"2025-03-08T03:15:20.013711Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Load the dataset\ndataset_dir = \"/kaggle/working/frames\"  # Directory where frames are saved\ndataset = datasets.ImageFolder(root=dataset_dir, transform=transform)\n\n# Split dataset into training and validation sets\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T03:19:35.841278Z","iopub.execute_input":"2025-03-08T03:19:35.841560Z","iopub.status.idle":"2025-03-08T03:19:35.868103Z","shell.execute_reply.started":"2025-03-08T03:19:35.841538Z","shell.execute_reply":"2025-03-08T03:19:35.867500Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import torch.nn as nn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T03:19:36.615213Z","iopub.execute_input":"2025-03-08T03:19:36.615489Z","iopub.status.idle":"2025-03-08T03:19:36.619038Z","shell.execute_reply.started":"2025-03-08T03:19:36.615466Z","shell.execute_reply":"2025-03-08T03:19:36.618300Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Load Vision Transformer (ViT) model\nmodel = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=2)\nmodel.to(device)\nmodel = nn.DataParallel(model)\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)\n\n# Training loop with early stopping\nnum_epochs = 20\nbest_val_accuracy = 0\npatience = 5\npatience_counter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct_train = 0\n    total_train = 0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total_train += labels.size(0)\n        correct_train += (predicted == labels).sum().item()\n\n    train_accuracy = 100 * correct_train / total_train\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n\n    # Validation\n    model.eval()\n    correct_val = 0\n    total_val = 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            total_val += labels.size(0)\n            correct_val += (predicted == labels).sum().item()\n\n    val_accuracy = 100 * correct_val / total_val\n    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n\n    # Save best model with early stopping\n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        torch.save(model.state_dict(), 'best_vit_model.pth')\n        patience_counter = 0\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(\"Early stopping due to no improvement.\")\n            break\n    scheduler.step()\n\nprint(f\"Best Validation Accuracy: {best_val_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T03:23:57.497527Z","iopub.execute_input":"2025-03-08T03:23:57.497828Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4420103e25424117849059cab2822cdc"}},"metadata":{}},{"name":"stdout","text":"Epoch [1/20], Loss: 0.5713, Training Accuracy: 68.79%\nValidation Accuracy: 76.77%\nEpoch [2/20], Loss: 0.3824, Training Accuracy: 81.35%\nValidation Accuracy: 83.94%\nEpoch [3/20], Loss: 0.3309, Training Accuracy: 84.35%\nValidation Accuracy: 85.09%\nEpoch [4/20], Loss: 0.2745, Training Accuracy: 86.32%\nValidation Accuracy: 85.24%\nEpoch [5/20], Loss: 0.2427, Training Accuracy: 88.11%\nValidation Accuracy: 86.25%\nEpoch [6/20], Loss: 0.2228, Training Accuracy: 88.28%\nValidation Accuracy: 87.41%\nEpoch [7/20], Loss: 0.2127, Training Accuracy: 88.31%\nValidation Accuracy: 86.76%\nEpoch [8/20], Loss: 0.1969, Training Accuracy: 89.11%\nValidation Accuracy: 87.77%\nEpoch [9/20], Loss: 0.1872, Training Accuracy: 89.71%\nValidation Accuracy: 87.26%\nEpoch [10/20], Loss: 0.1908, Training Accuracy: 89.60%\nValidation Accuracy: 87.19%\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score\n\n# Load the best model\nmodel.load_state_dict(torch.load('best_vit_model.pth', weights_only=True))\nmodel.eval()\n\nall_labels = []\nall_predictions = []\n\nwith torch.no_grad():\n    for images, labels in val_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n\n        all_labels.extend(labels.cpu().numpy())\n        all_predictions.extend(predicted.cpu().numpy())\n\n# Calculate classification metrics\nprint(\"Classification Report:\")\nprint(classification_report(all_labels, all_predictions, target_names=['Real', 'Manipulated']))\n\n# Optionally print accuracy separately\naccuracy = accuracy_score(all_labels, all_predictions)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T19:07:01.736866Z","iopub.execute_input":"2025-01-30T19:07:01.737224Z","iopub.status.idle":"2025-01-30T19:08:03.218465Z","shell.execute_reply.started":"2025-01-30T19:07:01.737195Z","shell.execute_reply":"2025-01-30T19:08:03.217437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Confusion matrix\ncm = confusion_matrix(all_labels, all_predictions)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Manipulated'], yticklabels=['Real', 'Manipulated'])\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T19:08:17.480988Z","iopub.execute_input":"2025-01-30T19:08:17.481833Z","iopub.status.idle":"2025-01-30T19:08:18.03524Z","shell.execute_reply.started":"2025-01-30T19:08:17.4818Z","shell.execute_reply":"2025-01-30T19:08:18.034243Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom PIL import Image\nimport cv2\nimport timm\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define image transformations (same as used during training)\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load the trained model\nmodel = timm.create_model('vit_large_patch16_224', pretrained=False, num_classes=2)\n# Load the trained model with weights_only=True for security\nmodel.load_state_dict(torch.load('best_vit_model.pth', weights_only=True))\nmodel.to(device)\nmodel.eval()\n\n# Function to process the video and classify each frame\ndef predict_video(video_path, model, transform, device):\n    cap = cv2.VideoCapture(video_path)\n    frame_count = 0\n    real_count = 0\n    manipulated_count = 0\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        frame_count += 1\n\n        # Convert frame to PIL Image and apply transformations\n        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n\n        # Make prediction\n        with torch.no_grad():\n            outputs = model(image)\n            _, predicted = torch.max(outputs, 1)\n        \n        if predicted.item() == 0:\n            real_count += 1\n        else:\n            manipulated_count += 1\n\n    cap.release()\n\n    # Final decision based on majority vote across all frames\n    if real_count > manipulated_count:\n        print(f\"Result: Real video ({real_count} real frames, {manipulated_count} manipulated frames)\")\n        return \"Real\"\n    else:\n        print(f\"Result: Manipulated video ({real_count} real frames, {manipulated_count} manipulated frames)\")\n        return \"Manipulated\"\n\n# Test the video\nvideo_path = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_original sequences/02__kitchen_still.mp4\"\nresult = predict_video(video_path, model, transform, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T19:08:35.15783Z","iopub.execute_input":"2025-01-30T19:08:35.15831Z","iopub.status.idle":"2025-01-30T19:09:40.167927Z","shell.execute_reply.started":"2025-01-30T19:08:35.158285Z","shell.execute_reply":"2025-01-30T19:09:40.167096Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_path = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_manipulated_sequences/DFD_manipulated_sequences/01_20__walking_and_outside_surprised__OTGHOG4Z.mp4\"\nresult = predict_video(video_path, model, transform, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T19:10:07.919339Z","iopub.execute_input":"2025-01-30T19:10:07.919652Z","iopub.status.idle":"2025-01-30T19:11:06.035183Z","shell.execute_reply.started":"2025-01-30T19:10:07.919627Z","shell.execute_reply":"2025-01-30T19:11:06.034166Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_path = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_original sequences/15__exit_phone_room.mp4\"\nresult = predict_video(video_path, model, transform, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T19:11:15.915203Z","iopub.execute_input":"2025-01-30T19:11:15.915523Z","iopub.status.idle":"2025-01-30T19:11:44.610376Z","shell.execute_reply.started":"2025-01-30T19:11:15.915497Z","shell.execute_reply":"2025-01-30T19:11:44.609359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_path = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_manipulated_sequences/DFD_manipulated_sequences/01_20__walking_and_outside_surprised__OTGHOG4Z.mp4\"\nresult = predict_video(video_path, model, transform, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T19:12:07.209299Z","iopub.execute_input":"2025-01-30T19:12:07.209582Z","iopub.status.idle":"2025-01-30T19:13:05.402149Z","shell.execute_reply.started":"2025-01-30T19:12:07.209559Z","shell.execute_reply":"2025-01-30T19:13:05.401134Z"}},"outputs":[],"execution_count":null}]}